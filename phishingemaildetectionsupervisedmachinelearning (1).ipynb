{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8502378,"sourceType":"datasetVersion","datasetId":5074342},{"sourceId":14503045,"sourceType":"datasetVersion","datasetId":9262978}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nimport os\nimport joblib\nimport warnings\nfrom typing import Dict, List, Tuple, Union, Any\nimport sys\n\n# Machine Learning imports\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n\n# Visualization (optional)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# NLTK\nfrom nltk.corpus import stopwords\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\n\n# Download NLTK data\ntry:\n    nltk.data.find('corpora/stopwords')\nexcept LookupError:\n    nltk.download('stopwords')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T05:20:43.973428Z","iopub.execute_input":"2026-01-15T05:20:43.973777Z","iopub.status.idle":"2026-01-15T05:20:43.981495Z","shell.execute_reply.started":"2026-01-15T05:20:43.973751Z","shell.execute_reply":"2026-01-15T05:20:43.980290Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\n\nfile_path = '/kaggle/input/phishing-email-dataset/CEAS_08.csv'\ninitial_df = pd.read_csv(file_path)\ninitial_df.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-15T05:20:56.262513Z","iopub.execute_input":"2026-01-15T05:20:56.262835Z","iopub.status.idle":"2026-01-15T05:20:57.246628Z","shell.execute_reply.started":"2026-01-15T05:20:56.262810Z","shell.execute_reply":"2026-01-15T05:20:57.245673Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              sender  \\\n0                   Young Esposito <Young@iworld.de>   \n1                       Mok <ipline's1983@icable.ph>   \n2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n3                 Michael Parker <ivqrnai@pobox.com>   \n4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n\n                                         receiver  \\\n0                     user4@gvc.ceas-challenge.cc   \n1                   user2.2@gvc.ceas-challenge.cc   \n2                   user2.9@gvc.ceas-challenge.cc   \n3  SpamAssassin Dev <xrh@spamassassin.apache.org>   \n4                   user2.2@gvc.ceas-challenge.cc   \n\n                              date  \\\n0  Tue, 05 Aug 2008 16:31:02 -0700   \n1  Tue, 05 Aug 2008 18:31:03 -0500   \n2  Tue, 05 Aug 2008 20:28:00 -1200   \n3  Tue, 05 Aug 2008 17:31:20 -0600   \n4  Tue, 05 Aug 2008 19:31:21 -0400   \n\n                                             subject  \\\n0                          Never agree to be a loser   \n1                             Befriend Jenna Jameson   \n2                               CNN.com Daily Top 10   \n3  Re: svn commit: r619753 - in /spamassassin/tru...   \n4                         SpecialPricesPharmMoreinfo   \n\n                                                body  label  urls  \n0  Buck up, your troubles caused by small dimensi...      1     1  \n1  \\nUpgrade your sex and pleasures with these te...      1     1  \n2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1     1  \n3  Would anyone object to removing .so from this ...      0     1  \n4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1     1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sender</th>\n      <th>receiver</th>\n      <th>date</th>\n      <th>subject</th>\n      <th>body</th>\n      <th>label</th>\n      <th>urls</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Young Esposito &lt;Young@iworld.de&gt;</td>\n      <td>user4@gvc.ceas-challenge.cc</td>\n      <td>Tue, 05 Aug 2008 16:31:02 -0700</td>\n      <td>Never agree to be a loser</td>\n      <td>Buck up, your troubles caused by small dimensi...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mok &lt;ipline's1983@icable.ph&gt;</td>\n      <td>user2.2@gvc.ceas-challenge.cc</td>\n      <td>Tue, 05 Aug 2008 18:31:03 -0500</td>\n      <td>Befriend Jenna Jameson</td>\n      <td>\\nUpgrade your sex and pleasures with these te...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Daily Top 10 &lt;Karmandeep-opengevl@universalnet...</td>\n      <td>user2.9@gvc.ceas-challenge.cc</td>\n      <td>Tue, 05 Aug 2008 20:28:00 -1200</td>\n      <td>CNN.com Daily Top 10</td>\n      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Michael Parker &lt;ivqrnai@pobox.com&gt;</td>\n      <td>SpamAssassin Dev &lt;xrh@spamassassin.apache.org&gt;</td>\n      <td>Tue, 05 Aug 2008 17:31:20 -0600</td>\n      <td>Re: svn commit: r619753 - in /spamassassin/tru...</td>\n      <td>Would anyone object to removing .so from this ...</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Gretchen Suggs &lt;externalsep1@loanofficertool.com&gt;</td>\n      <td>user2.2@gvc.ceas-challenge.cc</td>\n      <td>Tue, 05 Aug 2008 19:31:21 -0400</td>\n      <td>SpecialPricesPharmMoreinfo</td>\n      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"df = initial[['body', 'label']]\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T05:21:04.879983Z","iopub.execute_input":"2026-01-15T05:21:04.880336Z","iopub.status.idle":"2026-01-15T05:21:04.893618Z","shell.execute_reply.started":"2026-01-15T05:21:04.880311Z","shell.execute_reply":"2026-01-15T05:21:04.892553Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                                body  label\n0  Buck up, your troubles caused by small dimensi...      1\n1  \\nUpgrade your sex and pleasures with these te...      1\n2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...      1\n3  Would anyone object to removing .so from this ...      0\n4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>body</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Buck up, your troubles caused by small dimensi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\\nUpgrade your sex and pleasures with these te...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Would anyone object to removing .so from this ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"print('shape.........')\nprint(df.shape)\nprint('info.....')\nprint(df.info())\nprint('isNull......')\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T05:21:12.963140Z","iopub.execute_input":"2026-01-15T05:21:12.963492Z","iopub.status.idle":"2026-01-15T05:21:12.991780Z","shell.execute_reply.started":"2026-01-15T05:21:12.963463Z","shell.execute_reply":"2026-01-15T05:21:12.990803Z"}},"outputs":[{"name":"stdout","text":"shape.........\n(39154, 2)\ninfo.....\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 39154 entries, 0 to 39153\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   body    39154 non-null  object\n 1   label   39154 non-null  int64 \ndtypes: int64(1), object(1)\nmemory usage: 611.9+ KB\nNone\nisNull......\nbody     0\nlabel    0\ndtype: int64\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"#  Feature Extraction","metadata":{}},{"cell_type":"code","source":"class EmailFeatureExtractor:\n\n    keywords_dir = '/kaggle/input/keywordsfile/'\n    if keywords_dir not in sys.path:\n        sys.path.insert(0, keywords_dir)\n\n    from keywords import (\n        BUSINESS_KEYWORDS,\n        REAL_ESTATE_KEYWORDS,\n        PERSONAL_KEYWORDS,\n        TECHNICAL_KEYWORDS,\n        LEGAL_KEYWORDS,\n        ENERGY_KEYWORDS,\n        FINANCIAL_KEYWORDS,\n        URGENCY_KEYWORDS,\n        MALICIOUS_OFFER_KEYWORDS\n    )\n\n    def __init__(self, training_data=None):\n        self.stop_words = set(stopwords.words('english'))\n        self.exclusive_to_phishing = set()\n        self.exclusive_to_ham = set()\n        self.exclusive_phish_urls = set()\n        self.exclusive_ham_urls = set()\n\n        # If training data provided, learn exclusive patterns\n        if training_data is not None:\n            self.learn_exclusive_patterns(training_data)\n\n    def learn_exclusive_patterns(self, df):\n        print(\"Learning exclusive patterns from training data...\")\n\n        # 1: Extract words for Phishing (Label 1)\n        df_phishing = df[df['label'] == 1]\n        phishing_words = []\n        for body in df_phishing['body'].dropna():\n            phishing_words.extend(self.extract_clean_words(str(body)))\n        unique_phishing = set(phishing_words)\n\n        # 2: Extract words for Ham (Label 0)\n        df_ham = df[df['label'] == 0]\n        ham_words = []\n        for body in df_ham['body'].dropna():\n            ham_words.extend(self.extract_clean_words(str(body)))\n        unique_ham = set(ham_words)\n\n        # Words in Phishing that NEVER appear in Ham\n        self.exclusive_to_phishing = unique_phishing - unique_ham\n\n        # Words in Ham that NEVER appear in Phishing\n        self.exclusive_to_ham = unique_ham - unique_phishing\n\n        # URL extraction\n        temp_df = df.copy()\n        temp_df['extracted_url_list'] = temp_df['body'].apply(self.extract_urls)\n\n        phishing_urls = []\n        for url_list in temp_df[temp_df['label'] == 1]['extracted_url_list']:\n            phishing_urls.extend(url_list)\n        unique_phishing_urls = set(phishing_urls)\n\n        ham_urls = []\n        for url_list in temp_df[temp_df['label'] == 0]['extracted_url_list']:\n            ham_urls.extend(url_list)\n        unique_ham_urls = set(ham_urls)\n\n        self.exclusive_phish_urls = unique_phishing_urls - unique_ham_urls\n        self.exclusive_ham_urls = unique_ham_urls - unique_phishing_urls\n\n        print(f\"Learned {len(self.exclusive_to_phishing)} exclusive phishing words\")\n        print(f\"Learned {len(self.exclusive_to_ham)} exclusive ham words\")\n        print(f\"Learned {len(self.exclusive_phish_urls)} exclusive phishing URLs\")\n        print(f\"Learned {len(self.exclusive_ham_urls)} exclusive ham URLs\")\n\n    def extract_clean_words(self, text):\n        if not isinstance(text, str):\n            return []\n\n        # 1. Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n\n        # 2. Remove numbers and special characters\n        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n\n        # 3. Convert to lowercase\n        words = text.lower().split()\n\n        # 4. Filter out \"helping words\"\n        meaningful_words = [w for w in words if w not in self.stop_words and len(w) > 2]\n\n        return meaningful_words\n\n    def extract_urls(self, text):\n        \"\"\"Extract URLs from text\"\"\"\n        if not isinstance(text, str):\n            return []\n\n        url_pattern = r'https?://\\S+|www\\.\\S+'\n        urls = re.findall(url_pattern, text)\n        return urls\n\n    def detect_money_patterns(self, text):\n        \"\"\"Detect money patterns in text\"\"\"\n        if not text or pd.isna(text):\n            return 0, 0\n\n        text_lower = str(text).lower()\n\n        # Currency Symbols\n        currency_symbols = (\n            r\"\\$|Â£|â‚¬|Â¥|â‚¹|Ø‹|Æ’|â‚¼|Br|BZ\\$|\\$b|KM|P|Ð»Ð²|R\\$|áŸ›|â‚¡|kn|â‚±|KÄ|kr|Q|Â¢|Ft|â‚¨|â‚¦|C\\$|Gs|B\\/\\.|S\\/\\.|lei|â‚½|â‚ª|Rp|ï·¼|â‚­|Ð´ÐµÐ½|RM|â‚®\"\n        )\n\n        # Currency Names\n        currency_names = (\n            r\"dollar|doller|usd|us dollars?|bucks?|\"\n            r\"euro|eur|\"\n            r\"pound|gbp|\"\n            r\"rupee|rupees?|rs|inr|pkr|lkr|\"\n            r\"yen|jpy|yuan|rmb|\"\n            r\"riyal|rial|sar|qar|omr|\"\n            r\"dirham|aed|\"\n            r\"naira|ngn|\"\n            r\"peso|mxn|php|\"\n            r\"rube|rouble|rub|\"\n            r\"shekel|ils|\"\n            r\"ringgit|myr|\"\n            r\"won|krw|\"\n            r\"taka|bdt|\"\n            r\"franc|chf|\"\n            r\"lira|try|\"\n            r\"rand|zar\"\n        )\n\n        # Patterns\n        symbol_pattern = rf\"({currency_symbols})\\s*[\\d,]+\\.?\\d*\"\n        name_pattern = rf\"([\\d,]+\\.?\\d*\\s*)?({currency_names})\\s*[\\d,]+\\.?\\d*\"\n\n        standard_count = len(re.findall(symbol_pattern, text_lower)) + len(re.findall(name_pattern, text_lower))\n\n        # Obfuscated/text money\n        obfuscated_patterns = [\n            r'\\bgrand\\b', r'\\bk\\b', r'\\bmil\\b', r'\\bbil\\b',\n            r'money|cash|fund|payment|transfer|account',\n            r'win|prize|lottery',\n            r'usd|gbp|eur|jpy|inr|cad',\n        ]\n        obfuscated_count = sum(len(re.findall(p, text_lower)) for p in obfuscated_patterns)\n\n        return standard_count, obfuscated_count\n\n    def extract_malicious_patterns(self, text: str) -> dict:\n\n        if not isinstance(text, str):\n            text = \"\"\n\n        text_lower = text.lower()\n        words = text_lower.split()\n        total_chars = len(text)\n        total_words = len(words) if len(words) > 0 else 1\n\n        # 1. URL Features (Malicious emails often have many or suspicious URLs)\n        urls = re.findall(r'https?://\\S+|www\\.\\S+', text_lower)\n        url_count = len(urls)\n\n        # Check for numeric IPs (e.g., http://192.168.1.1)\n        ip_in_url = 1 if re.search(r'https?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', text_lower) else 0\n\n        # 2. Urgency and Malicious Keywords\n        urgency_score = sum(1 for word in words if word in self.URGENCY_KEYWORDS)\n        offer_score = sum(1 for word in words if word in self.MALICIOUS_OFFER_KEYWORDS)\n        business_score = sum(1 for word in words if word in self.BUSINESS_KEYWORDS)\n\n        # 3. Punctuation & Style (Phishing often uses extreme punctuation)\n        exclamation_count = text.count('!')\n        dollar_count = text.count('$')\n        caps_count = sum(1 for c in text if c.isupper())\n        caps_ratio = caps_count / total_chars if total_chars > 0 else 0\n\n        # 4. Email Header Deception\n        is_fake_reply = 1 if text_lower.startswith(('re:', 'fwd:')) else 0\n\n        # Compile Features\n        malicious_features = {\n            'text_length': total_chars,\n            'word_count': total_words,\n            'url_count': url_count,\n            'url_density': url_count / total_words,\n            'ip_in_url': ip_in_url,\n            'urgency_score': urgency_score,\n            'offer_score': offer_score,\n            'business_score': business_score,\n            'exclamation_density': exclamation_count / total_words,\n            'dollar_density': dollar_count / total_words,\n            'caps_ratio': caps_ratio,\n            'is_fake_reply': is_fake_reply,\n            'avg_word_length': total_chars / total_words\n        }\n\n        return malicious_features\n\n    def extract_features_from_email(self, email_body):\n \n        if not isinstance(email_body, str):\n            email_body = str(email_body)\n\n        features = {}\n\n        # 1. Exclusive word features\n        words = set(self.extract_clean_words(email_body))\n        features['has_exclusive_phish_word'] = 1 if (words & self.exclusive_to_phishing) else 0\n        features['has_exclusive_ham_word'] = 1 if (words & self.exclusive_to_ham) else 0\n\n        # 2. Exclusive URL features\n        urls = set(self.extract_urls(email_body))\n        features['has_exclusive_phish_url'] = 1 if (urls & self.exclusive_phish_urls) else 0\n        features['has_exclusive_ham_url'] = 1 if (urls & self.exclusive_ham_urls) else 0\n\n        # 3. URL count\n        features['url_count'] = len(urls)\n\n        # 4. Body length\n        features['body_len'] = len(email_body)\n\n        # 5. Money detection\n        standard, obfuscated = self.detect_money_patterns(email_body)\n        features['money_standard_count'] = standard\n        features['money_obfuscated_count'] = obfuscated\n\n        # 6. Temporal features\n        features['has_dates'] = 1 if re.search(\n            r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2}|'\n            r'\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{4}-\\d{2}-\\d{2}',\n            email_body, re.IGNORECASE\n        ) else 0\n\n        features['has_times'] = 1 if re.search(\n            r'\\b\\d{1,2}:\\d{2}\\s*[ap]m?|\\b\\d{1,2}:\\d{2}',\n            email_body, re.IGNORECASE\n        ) else 0\n\n        features['timestamp_count'] = features['has_dates'] + features['has_times']\n\n        # 7. Domain-specific features\n        domain_patterns = {\n            'is_business': '|'.join(self.BUSINESS_KEYWORDS),\n            'is_personal': '|'.join(self.PERSONAL_KEYWORDS),\n            'is_financial': '|'.join(self.FINANCIAL_KEYWORDS),\n            'is_legal': '|'.join(self.LEGAL_KEYWORDS),\n            'is_technical': '|'.join(self.TECHNICAL_KEYWORDS),\n            'is_energy': '|'.join(self.ENERGY_KEYWORDS),\n            'is_real_estate': '|'.join(self.REAL_ESTATE_KEYWORDS),\n        }\n\n        for feature_name, pattern in domain_patterns.items():\n            features[feature_name] = 1 if re.search(pattern, email_body, re.IGNORECASE) else 0\n\n        # 8. NEW: Malicious pattern features\n        malicious_features = self.extract_malicious_patterns(email_body)\n        features.update(malicious_features)\n\n        return features\n\n    def extract_features_from_dataframe(self, df):\n        print(\"Extracting features from DataFrame...\")\n\n        # Make a copy to avoid modifying original\n        df_features = df.copy()\n\n        # 1. Exclusive word features\n        def check_exclusive_words(text):\n            if not isinstance(text, str):\n                return 0, 0\n            words = set(self.extract_clean_words(text))\n            has_phish_word = 1 if (words & self.exclusive_to_phishing) else 0\n            has_ham_word = 1 if (words & self.exclusive_to_ham) else 0\n            return has_phish_word, has_ham_word\n\n        # 2. Exclusive URL features\n        def check_exclusive_urls(text):\n            if not isinstance(text, str):\n                return 0, 0\n            urls = set(self.extract_urls(text))\n            has_phish_url = 1 if (urls & self.exclusive_phish_urls) else 0\n            has_ham_url = 1 if (urls & self.exclusive_ham_urls) else 0\n            return has_phish_url, has_ham_url\n\n        # Apply word features\n        word_feats = df_features['body'].apply(check_exclusive_words)\n        df_features['has_exclusive_phish_word'] = word_feats.apply(lambda x: x[0])\n        df_features['has_exclusive_ham_word'] = word_feats.apply(lambda x: x[1])\n\n        # Apply URL features\n        url_feats = df_features['body'].apply(check_exclusive_urls)\n        df_features['has_exclusive_phish_url'] = url_feats.apply(lambda x: x[0])\n        df_features['has_exclusive_ham_url'] = url_feats.apply(lambda x: x[1])\n\n        # 3. Additional features\n        df_features['url_count'] = df_features['body'].apply(lambda x: len(self.extract_urls(str(x))))\n        df_features['body_len'] = df_features['body'].apply(lambda x: len(str(x)))\n\n        # 4. Money detection\n        money_results = df_features['body'].apply(self.detect_money_patterns)\n        df_features['money_standard_count'] = money_results.apply(lambda x: x[0])\n        df_features['money_obfuscated_count'] = money_results.apply(lambda x: x[1])\n\n        # 5. Temporal features\n        df_features['has_dates'] = df_features['body'].str.contains(\n            r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2}|'\n            r'\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{4}-\\d{2}-\\d{2}',\n            case=False, na=False\n        ).astype(int)\n\n        df_features['has_times'] = df_features['body'].str.contains(\n            r'\\b\\d{1,2}:\\d{2}\\s*[ap]m?|\\b\\d{1,2}:\\d{2}',\n            case=False, na=False\n        ).astype(int)\n\n        df_features['timestamp_count'] = df_features['has_dates'].astype(int) + df_features['has_times'].astype(int)\n\n        # 6. Domain features\n        domain_mappings = {\n            'is_business': self.BUSINESS_KEYWORDS,\n            'is_personal': self.PERSONAL_KEYWORDS,\n            'is_financial': self.FINANCIAL_KEYWORDS,\n            'is_legal': self.LEGAL_KEYWORDS,\n            'is_technical': self.TECHNICAL_KEYWORDS,\n            'is_energy': self.ENERGY_KEYWORDS,\n            'is_real_estate': self.REAL_ESTATE_KEYWORDS,\n        }\n\n        for feature_name, keywords in domain_mappings.items():\n            pattern = '|'.join(keywords)\n            df_features[feature_name] = df_features['body'].str.contains(\n                pattern, case=False, na=False, regex=True\n            ).astype(int)\n\n        # 7.Extract malicious patterns for each email\n        malicious_features_list = df_features['body'].apply(self.extract_malicious_patterns)\n\n        # Convert list of dicts to DataFrame and merge\n        malicious_features_df = pd.DataFrame(malicious_features_list.tolist())\n        df_features = pd.concat([df_features, malicious_features_df], axis=1)\n\n\n        return df_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T05:22:39.966146Z","iopub.execute_input":"2026-01-15T05:22:39.966479Z","iopub.status.idle":"2026-01-15T05:22:40.014740Z","shell.execute_reply.started":"2026-01-15T05:22:39.966455Z","shell.execute_reply":"2026-01-15T05:22:40.013526Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# MODEL TRAINING ","metadata":{}},{"cell_type":"code","source":"\n# SECTION 1: MODEL TRAINER \n\n\ndef train_model(model, X_train, y_train, X_test, y_test, model_name):\n    \"\"\"Train and evaluate a model\"\"\"\n    print(f\"\\n--- Training and evaluating {model_name} ---\")\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1]\n\n    print(f\"\\n{model_name} - Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    print(f\"\\n{model_name} - Confusion Matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n\n    roc_auc = roc_auc_score(y_test, y_proba)\n    print(f\"\\n{model_name} - ROC AUC Score: {roc_auc:.4f}\")\n\n    return roc_auc, model\n\n\n# SECTION 2: MAIN TRAINING FUNCTION \n\n\ndef train_phishing_model(df):\n\n    print(\"PHISHING DETECTION MODEL TRAINING\")\n\n    df = df[['body', 'label']].copy()\n\n    print(\"Step 2: Training feature extractor...\")\n    extractor = EmailFeatureExtractor(training_data=df)\n\n    print(\"Step 3: Extracting features...\")\n    df_features = extractor.extract_features_from_dataframe(df)\n\n\n\n    # Display some extracted features\n    print(\" Sample of extracted features:\")\n    feature_cols = [c for c in df_features.columns if c not in ['body', 'label']]\n    print(f\"   Total features: {len(feature_cols)}\")\n\n\n    # Step 4: Prepare data for training\n    print(\"Step 4: Preparing training data...\")\n\n    # Identify binary and continuous columns for scaling\n    binary_columns = []\n    continuous_count_columns = []\n\n    # Drop the 'extracted_url_list' column if it exists\n    if 'extracted_url_list' in df_features.columns:\n        df_features = df_features.drop(columns=['extracted_url_list'])\n\n\n    for col in df_features.columns:\n        if col in ['body', 'label']:\n            continue\n\n        if pd.api.types.is_numeric_dtype(df_features[col]):\n            unique_values = df_features[col].nunique()\n            if unique_values == 2 and set(df_features[col].unique()) == {0, 1}:\n                binary_columns.append(col)\n            elif unique_values > 2 or (unique_values == 2 and not (set(df_features[col].unique()) == {0, 1})):\n                continuous_count_columns.append(col)\n\n    print(f\"   Binary features: {len(binary_columns)}\")\n    print(f\"   Continuous features: {len(continuous_count_columns)}\")\n    # display(df_features.head())\n    # Scale continuous features\n    df_scaled = df_features.copy()\n    scaler = RobustScaler()\n\n    if continuous_count_columns:\n        df_scaled[continuous_count_columns] = scaler.fit_transform(df_scaled[continuous_count_columns])\n        print(f\"   Scaled {len(continuous_count_columns)} continuous features\")\n\n    # Step 5: Split data\n    X = df_scaled.drop(columns=['body', 'label'])\n    y = df_scaled['label']\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n\n    print(f\"Data split:\")\n    print(f\"   Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n    print(f\"   Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n\n\n        # Get model type\n    model_types = ['logistic', 'random_forest', 'xgboost', 'svm', 'gradient_boosting']\n    print(f\"\\nAvailable model types: {', '.join(model_types)}\")\n    model_type = input(\"Enter model type (default: logistic): \").strip().lower()\n\n    if not model_type or model_type not in model_types:\n        model_type = 'logistic'\n\n\n    # Step 6: Train model\n    print(f\"Step 5: Training {model_type} model...\")\n\n    model_map = {\n        'logistic': LogisticRegression(random_state=42, solver='liblinear', max_iter=1000),\n        'random_forest': RandomForestClassifier(random_state=42, n_estimators=100),\n        'xgboost': XGBClassifier(random_state=42, n_estimators=100, use_label_encoder=False, eval_metric='logloss'),\n        'svm': SVC(random_state=42, probability=True),\n        'gradient_boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)\n    }\n\n    if model_type not in model_map:\n        print(f\" Unknown model type: {model_type}\")\n        print(f\"   Available: {list(model_map.keys())}\")\n        model_type = 'logistic'\n\n    model = model_map[model_type]\n    model_name = model_type.replace('_', ' ').title()\n\n    # Train and evaluate\n    roc_auc, trained_model = train_model(model, X_train, y_train, X_test, y_test, model_name)\n\n    # Step 7: Save all components\n    print(f\"Step 6: Saving model components...\")\n\n    # Create a components dictionary\n    components = {\n        'extractor': extractor,\n        'model': trained_model,\n        'scaler': scaler,\n        'feature_columns': X_train.columns.tolist(),\n        'binary_columns': binary_columns,\n        'continuous_columns': continuous_count_columns,\n        'model_type': model_type,\n        'roc_auc': roc_auc\n    }\n\n    # Save to file\n    model_filename = f'phishing_model_{model_type}.joblib'\n    joblib.dump(components, model_filename)\n\n    print(f\" Model saved to: {model_filename}\")\n    print(f\"   ROC AUC: {roc_auc:.4f}\")\n    print(f\"   Features: {len(X_train.columns)}\")\n\n    # Step 8: Feature importance (if available)\n    if hasattr(trained_model, 'feature_importances_'):\n        print(f\"\\n Feature Importance (Top 10):\")\n        feature_importance = pd.DataFrame({\n            'feature': X_train.columns,\n            'importance': trained_model.feature_importances_\n        }).sort_values('importance', ascending=False)\n\n        print(feature_importance.head(10).to_string(index=False))\n\n    print(\" TRAINING COMPLETE!\")\n\n    return components\n\n\n# SECTION 3: MAIN EXECUTION BLOCK\n\n\ndef main():\n    train_phishing_model(df)\n\nif __name__ == \"__main__\":\n\n   main()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T05:32:46.248355Z","iopub.execute_input":"2026-01-15T05:32:46.248787Z","iopub.status.idle":"2026-01-15T05:38:44.251647Z","shell.execute_reply.started":"2026-01-15T05:32:46.248757Z","shell.execute_reply":"2026-01-15T05:38:44.250352Z"}},"outputs":[{"name":"stdout","text":"PHISHING DETECTION MODEL TRAINING\nStep 2: Training feature extractor...\nLearning exclusive patterns from training data...\nLearned 24261 exclusive phishing words\nLearned 112505 exclusive ham words\nLearned 3742 exclusive phishing URLs\nLearned 24773 exclusive ham URLs\nStep 3: Extracting features...\nExtracting features from DataFrame...\n Sample of extracted features:\n   Total features: 31\nStep 4: Preparing training data...\n   Binary features: 15\n   Continuous features: 14\n   Scaled 14 continuous features\nData split:\n   Training set: 31323 samples, 31 features\n   Test set: 7831 samples, 31 features\n\nAvailable model types: logistic, random_forest, xgboost, svm, gradient_boosting\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter model type (default: logistic):  \n"},{"name":"stdout","text":"Step 5: Training logistic model...\n\n--- Training and evaluating Logistic ---\n\nLogistic - Classification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00      3462\n           1       1.00      1.00      1.00      4369\n\n    accuracy                           1.00      7831\n   macro avg       1.00      1.00      1.00      7831\nweighted avg       1.00      1.00      1.00      7831\n\n\nLogistic - Confusion Matrix:\n[[3453    9]\n [   0 4369]]\n\nLogistic - ROC AUC Score: 0.9997\nStep 6: Saving model components...\n Model saved to: phishing_model_logistic.joblib\n   ROC AUC: 0.9997\n   Features: 31\n TRAINING COMPLETE!\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Mail Pridiction \n","metadata":{}},{"cell_type":"code","source":"class PhishingPredictor:\n\n    def __init__(self, model_path=None):\n\n        if model_path:\n            self.load_model(model_path)\n        else:\n            self.components = None\n            self.is_loaded = False\n\n    def load_model(self, model_path):\n\n        print(f\"Loading model from {model_path}...\")\n\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n        self.components = joblib.load(model_path)\n        self.is_loaded = True\n\n        print(f\" Model loaded successfully!\")\n        print(f\"   Model type: {self.components['model_type']}\")\n\n    def predict_single(self, email_text, threshold=0.5, return_features=False):\n        \"\"\"\n        Predict if a single email is phishing.\n\n        \"\"\"\n        if not self.is_loaded:\n            raise ValueError(\"Model not loaded. Call load_model() first.\")\n\n        # Step 1: Extract features\n        extractor = self.components['extractor']\n        features = extractor.extract_features_from_email(email_text)\n\n        # Step 2: Create DataFrame with correct column order\n        features_df = pd.DataFrame([features])\n\n        # Ensure all expected columns exist (fill missing with 0)\n        for col in self.components['feature_columns']:\n            if col not in features_df.columns:\n                features_df[col] = 0\n\n        # Reorder columns to match training\n        features_df = features_df[self.components['feature_columns']]\n\n        # Step 3: Scale continuous features\n        if self.components['continuous_columns']:\n            features_df[self.components['continuous_columns']] = self.components['scaler'].transform(\n                features_df[self.components['continuous_columns']]\n            )\n\n        # Step 4: Make prediction\n        model = self.components['model']\n        probability = model.predict_proba(features_df)[0]\n\n        # Step 5: Prepare result\n        phishing_prob = probability[1]\n        is_phishing = phishing_prob >= threshold\n\n        result = {\n            'is_phishing': bool(is_phishing),\n            'phishing_probability': float(phishing_prob),\n            'ham_probability': float(probability[0]),\n            'threshold_used': threshold,\n            'confidence': self._get_confidence_level(phishing_prob)\n        }\n\n\n        # Add features if requested\n        if return_features:\n            result['features'] = features\n\n        return result\n\n\n    def _get_confidence_level(self, probability):\n        \"\"\"Get confidence level based on probability\"\"\"\n\n        if probability < 0.3:\n            return \"Low\"\n        elif probability < 0.6:\n            return \"Medium\"\n        else:\n            return \"High\"\n\n    def analyze_email(self, email_text, verbose=True):\n\n        result = self.predict_single(email_text, return_features=True)\n\n        if verbose:\n            print(\"\\n\" + \"=\"*60)\n            print(\"ðŸ” EMAIL ANALYSIS REPORT\")\n            print(\"\\n\" + \"=\"*60)\n            # Display prediction\n            if result['is_phishing']:\n                print(f\"\\nðŸ”´ PHISHING DETECTED!\")\n                print(f\"   Probability: {result['phishing_probability']:.1%}\")\n                print(f\"   Confidence: {result['confidence']}\")\n            else:\n                print(f\"\\nðŸŸ¢ LEGITIMATE EMAIL\")\n                print(f\"   Legitimacy Probability: {result['ham_probability']:.1%}\")\n                print(f\"   Confidence: {result['confidence']}\")\n\n\n\n            # Display important features\n            if 'features' in result:\n                print(f\"\\nðŸ“Š Key Features:\")\n                features = result['features']\n                \n                # Show top 5 non-zero features\n                non_zero_features = {k: v for k, v in features.items() if v != 0}\n                sorted_features = sorted(non_zero_features.items(), key=lambda x: abs(x[1]), reverse=True)\n\n                for feat, val in sorted_features[:25]:  # Show top 10\n                    feat_display = feat.replace('_', ' ').title()\n                    print(f\"   â€¢ {feat_display}: {val}\")\n\n        return result\n        \ndef main():\n    \n    model_path = '/kaggle/working/phishing_model_logistic.joblib'\n    # Load predictor\n    predictor = PhishingPredictor(model_path)\n\n    # Prediction options\n    print(\"PREDICTION OPTIONS:\")\n    print(\"1. Predict single email\")\n\n    pred_choice = input(\"\\nEnter choice (1): \").strip()\n\n    if pred_choice == '1':\n        email = input(\"\\nEnter email text: \").strip()\n        result = predictor.analyze_email(email)\n\n    else:\n        print(\"Invalid choice.\")\n\nif __name__ == \"__main__\":\n\n   main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T05:55:33.030366Z","iopub.execute_input":"2026-01-15T05:55:33.030726Z","iopub.status.idle":"2026-01-15T05:55:55.204253Z","shell.execute_reply.started":"2026-01-15T05:55:33.030676Z","shell.execute_reply":"2026-01-15T05:55:55.202640Z"}},"outputs":[{"name":"stdout","text":"Loading model from /kaggle/working/phishing_model_logistic.joblib...\n Model loaded successfully!\n   Model type: logistic\nPREDICTION OPTIONS:\n1. Predict single email\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nEnter choice (1):  1\n\nEnter email text:  Buck up, your troubles caused by small dimension will soon be over! Become a lover no woman will be able to resist http://whitedone.com/come. Even as Nazi tanks were rolling down the streets, the dreamersphilosopher or a journalist. He was still not sure.I do the same.\n"},{"name":"stdout","text":"\n============================================================\nðŸ” EMAIL ANALYSIS REPORT\n\n============================================================\n\nðŸ”´ PHISHING DETECTED!\n   Probability: 100.0%\n   Confidence: High\n\nðŸ“Š Key Features:\n   â€¢ Body Len: 269\n   â€¢ Text Length: 269\n   â€¢ Word Count: 45\n   â€¢ Avg Word Length: 5.977777777777778\n   â€¢ Has Exclusive Phish Word: 1\n   â€¢ Url Count: 1\n   â€¢ Is Technical: 1\n   â€¢ Caps Ratio: 0.022304832713754646\n   â€¢ Url Density: 0.022222222222222223\n   â€¢ Exclamation Density: 0.022222222222222223\n","output_type":"stream"}],"execution_count":34}]}